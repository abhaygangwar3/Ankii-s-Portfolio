{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KtvSa75_IgUYuyrOOfIGOQYn4NoduKJZ",
      "authorship_tag": "ABX9TyM4QhOafMMRA3SUUKzjM/lq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhaygangwar3/Ankii-s-Portfolio/blob/master/ConvXGB_method_to_classify_the_OPG_images_as_per_the_gender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ConvXGB method to classify the OPG images as per the gender**"
      ],
      "metadata": {
        "id": "xio01e7EPR4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**\n"
      ],
      "metadata": {
        "id": "AWxlDe_vPaF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "C7iXsHubPeLG"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the dataset from separate .npz files\n",
        "def load_data(npz_path):\n",
        "    data = np.load(npz_path)\n",
        "    X = data['X']\n",
        "    y = data['y']\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "u93vE69QPzKO"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your .npz files (replace with the actual paths)\n",
        "train_npz_path = '/content/drive/MyDrive/Train.npz'\n",
        "val_npz_path = '/content/drive/MyDrive/Val.npz'\n",
        "test_npz_path = '/content/drive/MyDrive/Test.npz'"
      ],
      "metadata": {
        "id": "NOvkhJnBQAoK"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = np.load(train_npz_path)\n",
        "data_test = np.load(val_npz_path)\n",
        "data_val = np.load(test_npz_path)\n"
      ],
      "metadata": {
        "id": "uACjoQkZtHsR"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train, validation, and test sets\n",
        "images_train = data_train['images']\n",
        "labels_train = data_train['labels']\n",
        "images_val = data_val['images']\n",
        "labels_val = data_val['labels']\n",
        "images_test = data_test['images']\n",
        "labels_test = data_test['labels']"
      ],
      "metadata": {
        "id": "lCJRcLvIsngi"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images_train.shape)\n",
        "print(labels_train.shape)\n",
        "print(images_val.shape)\n",
        "print(labels_val.shape)\n",
        "print(images_test.shape)\n",
        "print(labels_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRdZTuRvu5hD",
        "outputId": "4cf6bb15-30fc-4cd9-cc7e-559f7ee4828d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(562, 360, 120, 3)\n",
            "(562,)\n",
            "(12, 120, 360, 3)\n",
            "(12,)\n",
            "(106, 360, 120, 3)\n",
            "(106,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Normalize the images (assuming they are grayscale)\n",
        "images_train = images_train / 255.0\n",
        "images_val = images_val / 255.0\n",
        "images_test = images_test / 255.0"
      ],
      "metadata": {
        "id": "JMT55P0pttLI"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Convert 3-channel (RGB) images to grayscale\n",
        "images_train_gray = np.array([cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY) for img in images_train]) # Change the image type to np.uint8\n",
        "images_val_gray = np.array([cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY) for img in images_val]) # Change the image type to np.uint8\n",
        "images_test_gray = np.array([cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY) for img in images_test]) # Change the image type to np.uint8\n",
        "\n",
        "# Now images are grayscale, we need to reshape them to have a single channel (for CNN input)\n",
        "# Add an extra dimension at the end (channels = 1 for grayscale images)\n",
        "images_train_gray = images_train_gray.reshape(-1, images_train.shape[1], images_train.shape[2], 1)\n",
        "images_val_gray = images_val_gray.reshape(-1, images_val.shape[1], images_val.shape[2], 1)\n",
        "images_test_gray = images_test_gray.reshape(-1, images_test.shape[1], images_test.shape[2], 1)\n",
        "\n",
        "print(\"New shape of images_train:\", images_train_gray.shape)  # Should be (562, 360, 120, 1)\n",
        "print(\"New shape of images_val:\", images_val_gray.shape)      # Should be similar for validation\n",
        "print(\"New shape of images_test:\", images_test_gray.shape)    # Similar for test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRFla2bJyYIq",
        "outputId": "ca75eeb7-0531-416f-a061-1c4816874d5c"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New shape of images_train: (562, 360, 120, 1)\n",
            "New shape of images_val: (12, 120, 360, 1)\n",
            "New shape of images_test: (106, 360, 120, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "labels_train_one_hot = to_categorical(labels_train, num_classes=2)\n",
        "labels_val_one_hot = to_categorical(labels_val, num_classes=2)\n",
        "labels_test_one_hot = to_categorical(labels_test, num_classes=2)\n",
        "\n",
        "print(f\"Shape of labels_train_one_hot: {labels_train_one_hot.shape}\")  # Should be (562, 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbNwrrbyE1Ry",
        "outputId": "80cc0b23-4c82-4ae5-8190-eb7ef5b19702"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels_train_one_hot: (562, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Reshape the data for CNN input (assuming grayscale images)\n",
        "img_size = (images_train.shape[1], images_train.shape[2])  # Assuming (samples, height, width) shape\n",
        "images_train = images_train.reshape(-1, img_size[0], img_size[1], 1)\n",
        "images_val = images_val.reshape(-1, img_size[0], img_size[1], 1)\n",
        "images_test = images_test.reshape(-1, img_size[0], img_size[1], 1)"
      ],
      "metadata": {
        "id": "_gYe84UstxpB"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))  # Dropout to prevent overfitting\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))  # Dropout after every MaxPooling layer\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))  # Dropout after every Conv block\n",
        "\n",
        "    # Adding one more convolutional block for higher capacity\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))  # Dropout before final dense layers\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))  # Increased the size of the dense layer\n",
        "    model.add(Dropout(0.5))  # Dropout before final dense layers\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "OLE3WGtkt09S"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'images_train shape: {images_train.shape}')\n",
        "print(f'labels_train shape: {labels_train.shape}')\n",
        "print(f'images_val shape: {images_val.shape}')\n",
        "print(f'labels_val shape: {labels_val.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1H4VBEZua-n",
        "outputId": "089c991a-09ca-4c49-8e18-c690c5581b6d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images_train shape: (1686, 360, 120, 1)\n",
            "labels_train shape: (562,)\n",
            "images_val shape: (36, 360, 120, 1)\n",
            "labels_val shape: (12,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN model\n",
        "cnn_model = create_cnn_model((img_size[0], img_size[1], 1))\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEBR0_Ost4LZ",
        "outputId": "13014c40-57ce-4ac0-e63f-4a199bc8a997"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# # Adjust the augmentation settings\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=5,      # Reduced rotation range\n",
        "#     width_shift_range=0.05,  # Smaller width shift\n",
        "#     height_shift_range=0.05, # Smaller height shift\n",
        "#     zoom_range=0.05,         # Smaller zoom\n",
        "#     horizontal_flip=True,   # Horizontal flip should help in gender classification\n",
        "#     fill_mode='nearest'     # Default fill mode\n",
        "# )\n"
      ],
      "metadata": {
        "id": "p2cPdD2B9mDz"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the CNN model on the train set and validate on the validation set\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "cnn_model.fit(\n",
        "    images_train_gray, labels_train_one_hot,\n",
        "    validation_data=(images_val_gray, labels_val_one_hot),\n",
        "    epochs=50, callbacks=[reduce_lr, early_stopping]  # Train for a few epochs without augmentation\n",
        ")\n",
        "\n",
        "# Then introduce augmentation gradually\n",
        "# cnn_model.fit(\n",
        "#     datagen.flow(images_train_gray, labels_train_one_hot, batch_size=32),\n",
        "#     validation_data=(images_val_gray, labels_val_one_hot),\n",
        "#     epochs=20,  # Continue with mild augmentation,\n",
        "#     callbacks=[reduce_lr, early_stopping]\n",
        "# )\n",
        "\n",
        "# cnn_model.fit(images_train_gray, labels_train_one_hot, epochs=20, batch_size=32, validation_data=(images_val_gray, labels_val_one_hot),\n",
        "#               callbacks=[reduce_lr, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KANqY3Myt4G8",
        "outputId": "bc082c15-9c4d-4b08-c14b-da9085a35877"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - accuracy: 0.5829 - loss: 0.6833 - val_accuracy: 0.7500 - val_loss: 0.6667 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4s/step - accuracy: 0.6408 - loss: 0.6578 - val_accuracy: 0.7500 - val_loss: 0.6555 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 4s/step - accuracy: 0.6402 - loss: 0.6341 - val_accuracy: 0.5833 - val_loss: 0.6477 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.6368 - loss: 0.6216 - val_accuracy: 0.7500 - val_loss: 0.6380 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.6591 - loss: 0.5820 - val_accuracy: 0.6667 - val_loss: 0.6922 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 4s/step - accuracy: 0.7126 - loss: 0.5669 - val_accuracy: 0.5000 - val_loss: 0.7657 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.7178 - loss: 0.5148 - val_accuracy: 0.4167 - val_loss: 0.9666 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 4s/step - accuracy: 0.7683 - loss: 0.4922 - val_accuracy: 0.4167 - val_loss: 0.9565 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.7181 - loss: 0.4862 - val_accuracy: 0.5833 - val_loss: 1.0065 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78b7a5229e70>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# # Create an image data generator with augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=10,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     zoom_range=0.1,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "# # Fit the generator to your training data\n",
        "# datagen.fit(images_train_gray)\n",
        "\n",
        "# Use the augmented data generator in model fitting\n",
        "# cnn_model.fit(\n",
        "#     datagen.flow(images_train_gray, labels_train_one_hot, batch_size=32),\n",
        "#     validation_data=(images_val_gray, labels_val_one_hot),\n",
        "#     epochs=20\n",
        "# )"
      ],
      "metadata": {
        "id": "yTNvXgtS46Qx"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Remove the last layer from CNN to extract features\n",
        "cnn_model.pop()  # Remove the final dense layer to use CNN as a feature extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4uN9pmK0M4k",
        "outputId": "651ab8ab-d094-460b-d325-f7c258e0075c"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Dense name=dense_7, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from train, validation, and test sets\n",
        "train_features = cnn_model.predict(images_train_gray)\n",
        "images_val_gray = images_val_gray.reshape((12, 360, 120, 1))\n",
        "val_features = cnn_model.predict(images_val_gray)\n",
        "test_features = cnn_model.predict(images_test_gray)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3dGKyyC0OZH",
        "outputId": "eb136c76-5abb-48e9-ec32-c22a34daa721"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Train XGBoost Classifier on Extracted Features\n",
        "# xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42)"
      ],
      "metadata": {
        "id": "kYdWOQZy0Q7p"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set up the parameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'n_estimators': [200],\n",
        "    'max_depth': [5],\n",
        "    'learning_rate': [0.01],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Grid search for best parameters\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "grid_search.fit(train_features, labels_train)\n",
        "\n",
        "# Best model after grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best XGBoost parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(test_features)\n",
        "accuracy = accuracy_score(labels_test, y_pred)\n",
        "print(f\"Test Accuracy after tuning: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr957PeRFhtN",
        "outputId": "d8114593-1e74-48c2-dc8e-9f48b708eb55"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best XGBoost parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Test Accuracy after tuning: 0.5943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost on CNN features\n",
        "# xgb_model.fit(train_features, labels_train)"
      ],
      "metadata": {
        "id": "SDDRNdMq0TKt"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate the Model on the Test Data\n",
        "# y_pred = xgb_model.predict(test_features)\n",
        "# accuracy = accuracy_score(labels_test, y_pred)\n",
        "# print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "96CLujXP0VcE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using different techniques to increase accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "AvgQLeeWgv-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load VGG16 pre-trained model + higher level layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(360, 120, 3))\n",
        "\n",
        "# Freeze the base model layers (so we don't train them initially)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the pre-trained base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add custom layers on top of it\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))  # Dense layer with 128 units\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Final classification layer (1 unit for binary classification)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyhh-xzyg3jZ",
        "outputId": "94cc233b-343e-4efe-8888-4d17df573d4a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load VGG16 pre-trained model + higher level layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(360, 120, 3))\n",
        "\n",
        "# Freeze the base model layers (so we don't train them initially)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the pre-trained base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add custom layers on top of it\n",
        "model.add(Flatten())\n",
        "\n",
        "# You can't access output_shape before the model has been built\n",
        "# Call model.build() or model.predict() to define the output shape\n",
        "model.build(input_shape=(None, 360, 120, 3))\n",
        "\n",
        "# Now you can print the output shape\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(Dense(128, activation='relu'))  # Dense layer with 128 units\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Final classification layer (1 unit for binary classification)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "images_val_mod = images_val.reshape((12, 360, 120, 3))\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Assuming images_train, labels_train, images_val, and labels_val are already loaded\n",
        "train_generator = datagen.flow(images_train, labels_train, batch_size=32)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=(images_val_mod, labels_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHG_LAlThHyv",
        "outputId": "2f46ba73-3799-4607-99a4-dfd2e1ee6cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 16896)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 19s/step - accuracy: 0.4450 - loss: 0.8977 - val_accuracy: 0.7500 - val_loss: 0.6702\n",
            "Epoch 2/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 18s/step - accuracy: 0.5054 - loss: 0.7574 - val_accuracy: 0.6667 - val_loss: 0.6785\n",
            "Epoch 3/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.5009 - loss: 0.7124 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some of the deeper layers of the base model\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Re-compile the model with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training (fine-tuning)\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=5,\n",
        "                    validation_data=(images_val, labels_val))\n"
      ],
      "metadata": {
        "id": "y04sKDsShLoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(images_test, labels_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "LoUFGhozhQiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}